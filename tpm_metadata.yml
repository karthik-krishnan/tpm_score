---
Developer workstation initial setup:
- Is largely ad-hoc, undocumented, manual, and takes several days/weeks to be setup
  correctly. No two developer workstations are identical in terms of structure.
- Is largely clear and reasonably well documented, but still requires quite a lot
  of painful manual setup. Developer workstations more or less identical and can be
  used by other team members with a little bit of effort.
- Is clear, well documented and maintained up-to-date. Following a series of mostly
  manual steps will result in a developer workstation that is mostly identical to
  another.
- Is clear, well documented and maintained up-to-date. Some portions of the setup
  are automated and result in identical workstation station state.
- Is clear, well documented and almost fully automated. Team members can work on any
  workstation without any loss in productivity
Developer workstation setup drift:
- Is fairly common. Developers have to manually make sure to keep their workstations
  up to date with the latest security patches, software and tools. Developers are
  unable to use other team members' workstations for project related work without
  significant loss of productivity
- Is fairly common. Security patches and other critical OS level fixes are applied
  automatically. Developers are responsible to keep project related software and tools
  up to date themselves. Developers are unable to use other team members' workstations
  for project related work without significant loss of productivity
- Is not that common. Teams have word of mouth conventions on where project related
  software can be found. Getting up to date with the latest software is easy. Developers
  can use other team members' with minimal loss in productivity.
- Is uncommon. Automation exists to get up-to-date with most if not all software.
  Developers can easily work on each others' workstations with almost no loss in productivity.
- Is alien. Developers use ephemeral workstations with no environment based or personal
  artifacts being required. Changes to workstation setup are scripted and automated.
Developer workstation to production parity:
- Developer workstations have almost no parity with formal/production environments.
  Any testing or verification related activities are almost always required to be
  completed in environments external to the developer's workstation.
- Developer workstations have little parity with formal/production environments. However,
  basic testing. verification related activities can be done on a developer workstation.
  However, end-to-end verification is almost always required to be completed in environments
  external to the developer's workstation.
- Developer workstations have little parity with formal/production environments. However,
  testing maturity is high enough to enable most testing, verification related activities
  to be completed on the developer workstation itself. Integration and end-to-end
  testing still happen on formal environments
- Developer workstations are able to simulate formal/production environments with
  the help of
- Provisioning of private production-like environments along with test data is scripted.
  End-to-end testing is very cheap and easy in terms of effort.
Integrated Development Environment (IDE):
- Developers use an IDE of their choice. Within the same team it is common to have
  team members use different IDEs. These IDEs are configured in custom ways for code
  formatting, static analysis, security analysis, build settings, etc.
- Developers on the same team use the same IDE. However, there are variations in terms
  for settings for code formatting, static analysis, security analysis, build settings,
  etc.
- Developers are free to use an IDE of their choice. However, settings for code formatting,
  static analysis, security analysis, build settings, etc. are shared informally.
  These settings are also enforced as part of the build.
- Developers are free to use an IDE of their choice. However, settings for static
  analysis, security analysis, build settings, etc. are shared formally.
- Developers on a team are encouraged to use the same IDE. Settings for code formatting,
  static analysis, security analysis, build settings, etc. are shared formally. Furthermore,
  settings are inherited from a common repository.
New application starters:
- Creating new applications requires careful assembly and time from senior members
  of the team. Choice of third party dependencies and their versions is made on a
  ad-hoc manner
- Creating new applications is done through copying existing applications and making
  changes as necessary. While this works most of the time, it results in unexpected
  errors occasionally.
- Creating new applications is done using standard scaffold creators on the public
  internet.
- Creating new applications is done using in-house tested extensions derived from
  standard scaffold creators. Maintaining dependencies and standards after the scaffold
  is created is left to individual teams.
- Creating new applications is done using in-house tested extensions derived from
  standard scaffold creators. Upgrading to use the latest security patches, library
  versions
Production Debugging:
- Debugging production issues requires special access. Most team members have long
  term access to production infrastructure to ease diagnosis. Getting such access
  is undocumented and a long drawn process.
- Debugging production issues requires special access. However, the process to request
  access is well documented.
- Debugging production issues does not require special access to most systems (esp.
  in-house built). These systems make enough observability information to make debugging
  easy without exposing sensitive data.
- Debugging production issues does not require special access to any systems. These
  systems make enough observability information to make debugging easy without exposing
  sensitive data.
- All teams are actively encouraged to deploy applications that make observability
  information available in standard ways. Production debugging of distributed systems
  is very easy through the use of correlation identifiers that span the entire user
  flow.
Test Environments:
- There is a single manually provisioned, shared, long running regression test environment
  where all sorts of testing happens.
- There are manually provisioned, shared, long running test environments for specific
  purposes e.g. regression, hot fix etc.
- Each team owns a "dev" environment where they are able to experiment and deploy
  on-demand, in addition to the formal testing environments above.
- Teams are allowed and able to provision ephemeral test environments, but do not
  have the ability to generate the required test data to execute complex scenarios.
  Such environments are limited to testing "simple" scenarios.
- Teams are allowed and able to provision ephemeral test environments along with the
  necessary test data required to test complex scenarios.
Code Review Strategy:
- No formal or informal code reviews happen. Everyone is expected to know and do the
  right thing.
- Code reviews happen, but they are informal and done on an ad-hoc basis.
- Code reviews happen formally, but most review comments could have been automated.
  So code reviews usually only result in stylistic improvements, if any.
- Code reviews happen formally, most stylistic comments are taken care of using static
  analysis tools prior to a human review. Code review comments, however are not preserved
  for context.
- Code reviews happen formally, most stylistic comments are taken care of using static
  analysis tools prior to a human review. Code review comments are persisted to provide
  context.
Continuous Integration:
- Check-ins happen on a long running private branch and remain there until story/feature
  completion. Integration to the mainline usually takes several weeks or months..
- Check-ins initially happen on a private branch and remain there for the duration
  of a development sprint/iteration. Integration to the mainline happens every 2 to
  4 weeks.
- Check-ins and builds happen on a private branch. Features are usually fine grained
  to finish within a few days. Integration happens every few days.
- Check-ins and CI happen on a short-lived private branch to satisfy a pull-request
  workflow. A branch-by-abstraction style is used to turn off incomplete work. Integration
  is truly continuous.
- Check-ins and CI happen on the mainline. A branch-by-abstraction style is used to
  turn off incomplete work. Integration is truly continuous.
Code Standards:
- There are no formal code standards provided or enforced. Teams are free to use a
  code formatting tool of their choice if they so wish.
- There are code standards provided. However, these standards are not actively maintained
  to keep up with newer language features. Teams use these in an ad-hoc manner.
- Code standards are provided and actively maintained to keep up with updates to a
  variety of languages. Teams are encouraged to enforce these rules
- Code standards are provided and actively maintained. These standards are enforced
  as part of the build and will result in failures if rules are violated.
- Code standards are provided and actively maintained. IDE and build support is made
  available to allow adhering to standards seamlessly.
Static Analysis Tools:
- There are no formal static analysis prescribed or enforced. Teams are free to use
  static analysis tools of their choice if they so wish.
- There are static analysis tools prescribed. However, the rules for these tools are
  not actively maintained to keep up with newer language and/or library features.
  Teams use these in an ad-hoc manner.
- Static analysis tools are prescribed and actively maintained to keep up with updates
  to a variety of languages and libraries. Teams are encouraged to enforce these rules.
- Static analysis tools are prescribed and actively maintained. The rules for these
  tools are enforced as part of the build and will result in failures if rules are
  violated.
- Static analysis tools are prescribed and actively maintained. IDE and build support
  is made available to allow adhering to standards seamlessly.
Local Builds:
- It is not possible to build your application(s) in their entirety on local developer
  workstations.
- Local builds are possible, but require skipping certain phases in the build, for
  e.g. integration tests.
- Local builds are possible after dependency artifacts have been downloaded and cached
  locally.
- Local builds before check-in are encouraged, but take a long time to execute successfully.
  Furthermore, successful local builds still require monitoring the build after check-in
  because environmental issues are common.
- Local builds before check-in are encouraged, take a small amount of time to execute
  successfully. Successful local builds are a reliable indicator of build quality.
Build Stability:
- The build process does not include adequate testing. Builds are stable, but usually
  require a significant amount of manual testing to certify quality.
- The build process does include various types of automated testing. But these tests
  are time-consuming and flaky. Hence they require a significant amount of manual
  testing to certify quality.
- The build process includes a good mix of unit, integration, functional and acceptance
  tests, and these tests are mostly green. However,  a significant amount of manual
  testing is still needed to certify quality - primarily because of poor visibility
  into the quality of the tests that run as part of the build.
- The build process includes a good mix of unit, integration, functional and acceptance
  tests, and these tests are mostly green. Additional manual testing is required for
  scenarios not covered through automated testing.
- The build process includes a good mix of unit, integration, contract, functional
  and acceptance tests, and these tests are mostly green. Manual testing only used
  for exploratory testing.
Failed Builds:
- Failed builds remain red for prolonged periods of time. Check-ins on top of failed
  builds are fairly common. There is no clear ownership assignment for failed builds..
- Failed builds remain red for prolonged periods of time. Usually someone takes ownership,
  but fixing red builds takes a long time.
- Failed builds for prolonged periods of time are inevitable. Developers are unable
  to run stages in the pipeline locally and there isn't enough debugging information
  readily available.
- Failed builds are not that common. But when failures do happen, other team members
  wait for the build to be fixed. Team members avoid checking in on top of a red build.
- Failed builds are not that common. When they do occur, team members are empowered
  to rollback the cause of the build failure if they are not fixed by the owner.
Automated Unit Testing:
- Limitations in application design do not allow effective unit testing. Even when
  unit tests are written - it is usually done after the fact and coverage is too low
  for the process to be effective.
- Unit tests are written, mostly after production code, and coverage is also reasonable,
  but they are not effective as they don't catch many bugs. Higher, more expensive
  levels of testing are always required to establish codebase health.
- Unit tests are written, mostly after production code and coverage is also pretty
  high. They are reasonably effective in catching a few bugs, but higher levels of
  testing by a QA team external to the development team is still used to certify application
  functionality.
- Unit tests are written, with some tests being written before production code and
  coverage being pretty high, but the practice is not trusted enough to yield consistent
  benefits.
- Test-driven development is actively practiced. There is a high level of trust in
  the unit testing practice. Almost always, most testing is accomplished through unit
  testing and QA team members verify that required scenarios are covered as opposed
  to duplicating effort in more expensive tests.
Automated Integration Testing:
- Integration Testing relies on expensive environment and application dependencies.
  This makes this class of tests very flaky and ineffective. Consequently, teams use
  this class of tests very sparingly.
- Integration Testing relies on the creation of mocks and stubs for dependent applications
  and services. While these tests provide some value, changes to dependent implementations
  cause tests to pass while failing when run against real implementations. While this
  reduces the amount of flakiness, it sometimes results in false positives as well.
- Integration Testing relies on a mix of real (for internal) and stubbed (for external)
  dependencies. While this style sometimes provides a reasonable middle ground, investment
  in this class of tests is still not deep, given that they are still expensive to
  set up and run.
- Integration Testing is mostly used as an augment to unit testing with only a few
  tests being written against this class of tests. This provides a happy mix of the
  faster, less expensive unit tests vs. slower, more expensive integration tests
- Integration Testing relies on a set of mocks provided by the producer of the dependency.
  In addition to using this as merely an augment to unit tests, it also provides a
  good amount of reliability when running these tests.
Automated Contract Testing:
- Contract tests are written by very few teams, if any, primarily because teams are
  generally unaware of this class of tests.
- API contracts are driven primarily by provider teams with little real-world knowledge
  of client team requirements. This means that client teams expend a lot of effort
  writing transformation, business logic themselves.
- Contract tests are written by client teams against real implementations of providing
  services. This makes them equivalent to integration tests, reducing their utility
  and efficacy.
- Contract tests are written by client teams and contributed to the producer teams.
  Client teams use these stubs for functional, acceptance and other expensive forms
  of testing. These tests when run successfully in the producer's build pipeline result
  in stubs that the producer team publishes for client teams to use. However, this
  practice is not common yet.
- Contract tests are written by client teams and contributed to the producer teams.
  These tests when run successfully in the producer's build pipeline result in stubs
  that the producer team publishes for client teams to use. Client teams use these
  stubs for functional, acceptance and other expensive forms of testing. This is a
  standard practice across most if not all teams.
Automated Functional Testing:
- Automated functional testing is not practiced at all. All functional testing happens
  in the form of manual testing.
- Automated functional testing is practiced sporadically, however a majority of functional
  testing is manual.
- Automated functional testing is practiced by most teams, however, efficacy is still
  questionable because these tests are flaky. A round of manual testing always follows
  automated functional testing to certify build quality.
- Automated functional testing is practiced by most teams, and there is a good level
  of maturity among the teams. However, functional testing does not take into account
  the amount of testing that may have happened at lower levels. A lot of the functional
  testing effort could be reduced if more attention was paid to coverage already achieved
  at lower levels of automated testing.
- Automated functional testing is a standard practice for most (if not all) teams.
  Automated functional testing is reserved only for the most important scenarios and
  more importantly, only for those scenarios that have not already been tested at
  lower levels of testing.
Automated Acceptance Testing:
- Automated acceptance testing is not practiced at all. All acceptance testing happens
  in the form of manual testing.
- Automated functional testing is practiced sporadically, however a majority of acceptance
  testing is manual.
- Automated acceptance testing is practiced by most teams, however, efficacy is still
  questionable because these tests are flaky. A round of manual testing always follows
  automated acceptance testing to certify build quality.
- Automated acceptance testing is practiced by most teams, and there is a good level
  of maturity among the teams. However, acceptance testing does not take into account
  the amount of testing that may have happened at lower levels. A lot of the acceptance
  testing effort could be reduced if more attention was paid to coverage already achieved
  at lower levels of automated testing.
- Automated acceptance testing is a standard practice for most (if not all) teams.
  Automated acceptance testing is reserved only for the most important scenarios and
  more importantly, only for those scenarios that have not already been tested at
  lower levels of testing.
Performance & Scale Testing [Capacity Testing]:
- Not practiced
- Practiced, but only on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Performance & Scale Testing [Load Testing]:
- Not practiced
- Practiced, but only on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Performance & Scale Testing [Stress Testing]:
- Not practiced
- Practiced, but only on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Performance & Scale Testing [Soak Testing]:
- Not practiced
- Practiced, but only on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Performance & Scale Testing [Spike Testing]:
- Not practiced
- Practiced, but only on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Performance & Scale Testing [Volume Testing]:
- Not practiced
- Practiced, but only on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Performance & Scale Testing [Chaos Testing]:
- Not practiced
- Practiced, but only on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Manual Testing:
- Manual testing is the primary form of testing used to certify build quality. This
  is time consuming, error-prone, undocumented and mostly ad-hoc.
- Manual testing is the primary form of testing used to certify build quality. Although
  time consuming and error-prone, test cases and executions are documented.
- Manual testing is used as an augment to other forms of automated testing, however,
  it still remains the primary form of build quality.
- Manual testing is used as an augment to other forms of automated testing. Other
  forms of automated testing are consulted to make sure manual test cases do not repeat
  effort already spent during automated testing.
- Manual testing is used sparingly, only when automated testing is not feasible or
  too cumbersome. There are conscious efforts to intentionally increase automation
  all the time.
Test Data Management:
- There is no formal test data management strategy in place. Test data is mostly gathered
  organically as a side effect of testing efforts. If the regression test environment
  is lost, significant effort will be needed to restore testing productivity.
- Test data management mostly constitutes copying subsets of production data, which
  is repurposed for testing. However, this test data is not anonymized, so it might
  contain personally identifiable information.
- Test data management mostly constitutes copying anonymized subsets of production
  data.
- Test data management constitutes activities such as test case design, test data
  preparation using a combination of data generation script and copying anonymized
  production data subsets.
- Test data management constitutes activities such as test case design, test data
  preparation primarily using data generation scripts which are versioned just like
  any other piece of source code. These data generation scripts are used when provisioning
  new environments or before starting any form of functional, acceptance testing.
Refactoring:
- Functional features take too high a priority. There is no time allocated for refactoring.
- Refactoring is encouraged, but is usually deemed too dangerous. Hence very little
  to no refactoring actually occurs.
- Refactoring is encouraged, potential candidates are identified, and the low hanging
  fruit items are dealt with. Larger initiatives are deemed to dangerous and left
  alone.
- Refactoring is encouraged, usually as a task/story distinct from functional development
- Refactoring happens naturally as part of all functional development. Usually, there
  is no need to allocate time specifically for refactoring activities.
Packaging Format:
- EAR, WAR style into a shared application container with multiple applications/services
  running in the same process space.
- Mutable deployment target with configuration management software e.g. puppet, chef,
  etc.
- Immutable, versioned virtual machine(VM) image e.g. VMDK, AMI etc.
- Immutable, versioned container image e.g. docker, rocket etc.
- Versioned compound container image groups e.g. helm, docker-compose etc.
Deployment Style:
- Manual with downtime and outage window
- Automated with downtime with outage window
- Automated with zero downtime - blue green
- Automated with zero downtime - rolling or canary
- Automated with zero downtime - combination of blue-green and rolling
Business Continuity (Disaster Recovery):
- Active passive with MTTR of 24 hours or more
- Active passive with MTTR between 12 and 24 hours
- Active passive with MTTR between 6 and 12 hours
- Active passive with MTTR between 2 to 6 hours
- Active active with zero downtime
Deployment Frequency:
- Monthly or even less frequently
- Weekly or more
- Almost Daily
- Daily
- On-demand, several times a day. Any time functionality is ready and the business
  wants to release it.
Deployment Window:
- Deployments can only happen during low traffic "windows", because there is downtime
  when switching to the new version
- Deployments can only happen during low traffic "windows", because of a poor track
  record with failed (customer impacting) deployments.
- Deployments can happen anytime during the day, but continue to happen only during
  low traffic "windows" due to inertia.
- Deployments can happen anytime during the day, but continue to happen only during
  specific times of the day due to a lack of approvers, deployment operations personnel
  etc.
- Deployments can and happen anytime during the day. The business/product stakeholder
  is in complete  control of when deployments should happen.
Deployment Moratorium:
- There is always an extended, pre-emptive deployment freeze during peak periods (e.g.
  Thanksgiving, Christmas etc.) - no negotiations - except in case of critical bug
  fixes or
- There is always an extended, pre-emptive deployment freeze during peak periods,
  but deployments are possible with proper justifications.
- There are short deployment freeze during peak periods, but these are merely a formality.
  Deployments usually are done as and when necessary.
- There are no deployment freezes at any time of the year. However, deployment frequency
  slows during peak periods due to a lack of availability of key personnel (business
  and operations).
- Teams are free to deploy whenever they wish as long as the product/business team
  members are comfortable. Deployment is not a function of IT constraints, simply
  a business decision.
Deployment Target Host:
- Manually provisioned, long-running bare metal servers.
- Manually provisioned, long-running virtual machine servers.
- Programmatically provisioned, long running, mutable virtual machine servers.
- Programmatically provisioned, ephemeral, immutable virtual machine servers.
- Programmatically provisioned, ephemeral, immutable containers or serverless functions.
Datastore Style:
- Shared database server with public access to all schemas
- Shared database server with restricted access to individual schemas
- Shared database server with application access restricted to application owned schema
- Private database server with application access restricted to single application
  - database technology restricted to those currently supportable from an operational
  perspective.
- Private database server with application access restricted to single application
  - use of database technology appropriate for problem is permitted.
Application configuration:
- Is embedded within application deployment artifacts for a fixed set of environments.
  New environment(s) or changes to application configuration require an application
  rebuild.
- Is embedded within application artifacts. Environment specific configuration parameters
  can be overridden without requiring an application rebuild. However, overrides are
  not audited. Changes to configuration require an application redeployment.
- Is externalizable from application artifacts. But there is no operationalized mechanism
  to do so. Teams continue to require rebuilds when a change to application configuration
  is required.
- Is externalized from application configuration. Teams making use of externalized
  configuration use custom (including homegrown) methods to do so.
- Is externalized from application configuration. All teams use a consistent method
  to make changes to application configuration. All changes are traceable to a source
  code check-in.
Feature Toggles:
- Are not used to turn on/off application functionality dynamically.
- Are embedded within application deployment artifacts. Changes to toggles require
  an application rebuild.
- Are externalized from application deployment artifacts. Individual teams use custom
  methods to implement toggle functionality. Turning off functionality reliably requires
  intimate knowledge of application internals.
- Are externalized from application deployment artifacts. Teams use a consistent approach
  to toggles, but toggling off functionality still requires knowledge of application
  internals.
- Are externalized from application deployment artifacts. Toggles are defined at a
  business feature level
Release Notes:
- Release notes are not compiled or shared.
- Release notes are manually compiled on demand for important milestones.
- Release notes are manually compiled before every release.
- A subset of the release notes are compiled as part of the build pipeline.
- All changes and release notes are compiled as part of the build pipeline.
API Gateway:
- API gateway is not used as an explicit pattern.
- The use of API gateway is explicitly discouraged to avoid it becoming a home of
  excessive business logic.
- API Gateway is used as a single entry point for all consumers. It acts as a simple
  proxy for all downstream services and as a switch to enable zero downtime deployments.
- API Gateway is used as a single entry point for all consumers. It is used to expose
  different variations of the same API to different consumers and as a switch to enable
  zero downtime deployments.
- API Gateway is used as a single entry point for all consumers for value added services
  such as authentication, authorization and resilience patterns such as circuit breaker,
  rate limiting, throttling etc.
API Versioning:
- No explicit strategy is used for API versioning. API Consumers are notified if backwards
  incompatible changes are made and are expected
- API versioning is used very sparingly. In most cases, changes are made in a backwards
  compatible manner. When backwards incompatible changes are required to be made,
  new APIs are created.
- URL based API versioning is used at all times. Consumers are expected to include
  a version identifier in the request at all times.
- Header-based API versioning is used at all times. Consumers are expected to include
  a version identifier in the request at all times.
- Header-based API versioning is used when multiple versions of an API need to co-exist.
  Consumers not including an explicit version identifier are returned responses from
  the latest version.
API Documentation:
- API documentation is largely missing or ignored. API clients are required to consult
  producers' source code to look for usage patterns.
- API documentation is produced at the start of a project, but is not maintained up-to-date.
  Hence is used sparingly by consuming teams.
- API documentation is produced and all efforts are made to keep it up-to-date with
  with implementation changes. However, documentation does experience drift from implementation
  because it is maintained separately from the implementation.
- API documentation is produced and kept in sync with implementation at all times.
  Documentation and implementation are produced from the same source repository.
- API documentation and examples are produced and kept in sync with implementation
  at all times. Documentation is generated from tests and published as an artifact
  of the build. All documentation is published on a central API management portal
  for easy discoverability and access.
API Standards and Style Guides:
- No explicit API standards are published or prescribed. Individual teams use conventions
  as they see fit.
- API standards are published, but change too often, making adoption a bit of a hit
  or miss for both producers and consumers.
- API standards are published and are stable, but too many competing priorities have
  meant that adoption has been a hit or miss for both producers and consumers. New
  APIs follow the style guide pretty accurately.
- API standards are published and are stable. New APIs follow the style guide pretty
  accurately. There is a clear path defined to migrate existing APIs to the new standard.
- API standards are published and are stable. Most (if not all) APIs have adopted
  the style guide pretty accurately.
Architecture Decisions:
- Architecture decisions are made on an ad-hoc basis with individual teams responsible
  for all major decisions. There is little to no record maintained for decisions made.
- Architecture decisions are made by a central committee outside of the team and pushed
  to teams for adoption.
- Architecture decisions are initiated by teams working on a problem. These decisions
  are reviewed with a central committee to ensure standards are adhered to.
- Architecture decisions are initiated by teams working on a problem. These decisions
  are reviewed with a central committee to ensure standards are adhered to. Decisions
  made are captured and persisted in a lightweight manner.
- A pattern catalog of historic architecture decisions is consulted prior to large
  projects/undertakings. Teams works with an embedded representative from an architecture
  forum to expedite decisions from both a domain and technology perspective. These
  decisions are reviewed with a central committee to ensure standards are adhered
  to. Decisions made are captured and persisted in a lightweight manner.
Architecture Conformance:
- The primary means to enforce architecture decisions is through documents and accompanying
  literature.
- The primary means to enforce architecture decisions is through architecture and
  code reviews.
- Architecture decisions are enforced through a combination of documentation, reviews
  and static analysis observations.
- Architecture decisions are enforced through a combination of documentation, reviews
  static analysis observations. In addition, some architecture decisions are enforced
  using a set of fitness functions that are integrated as part of the build.
- Architecture decisions are enforced through a combination of documentation, reviews
  static analysis observations. In addition, a majority of architecture decisions
  are enforced using a set of fitness functions that are integrated as part of the
  build.
Architectural Styles:
- Big Ball of Mud Monolith
- Structured Monolith
- Service Oriented Microservices
- Event-Driven Microservices
- Serverless Functions
Resilience Patterns [Circuit Breakers]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Resilience Patterns [Client-side Load Balancers]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Resilience Patterns [Service Discovery]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Resilience Patterns [API Fallbacks]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Resilience Patterns [Rate Limiting]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Resilience Patterns [Throttling]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Resilience Patterns [Bulkheads]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Resilience Patterns [Timeouts]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Resilience Patterns [Caching]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested for meeting SLAs
Observability Patterns [Log Aggregation]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested to meet SLAs
Observability Patterns [Monitoring]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested to meet SLAs
Observability Patterns [Distributed Tracing]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested to meet SLAs
Observability Patterns [Health Checks]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested to meet SLAs
Observability Patterns [Alerting]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested to meet SLAs
Observability Patterns [Dashboards]:
- Not used
- Used sporadically by individual teams
- Pattern documented, with varying implementation
- Implementation standardized
- Tested to meet SLAs
Security Testing [Static Analysis Security Testing (SAST)]:
- Not practiced
- Practiced, on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Security Testing [Dynamic  Analysis Security Testing (DAST)]:
- Not practiced
- Practiced, on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Security Testing [Interactive  Analysis Security Testing (IAST)]:
- Not practiced
- Practiced, on an ad-hoc basis
- Practiced, but only before significant milestones (like a large cross-functional
  release)
- Practiced periodically, but not part of the pipeline
- Practiced consistently and automated as part of the pipeline
Data at Rest Security:
- Data stored at rest is stored in human readable form.
- Most data stored at rest is stored in human readable form. However, sensitive pieces
  of information are encrypted before they are written to disk. Teams are required
  to make encryption library choices on a case-by-case basis.
- Most data stored at rest is stored in human readable form. However, sensitive pieces
  of information are encrypted before they are written to disk. Encryption libraries
  are standardized across teams.
- Data stored at rest is encrypted before it is written to disk. Responsibility for
  key management is an individual team responsibility.
- Data stored at rest is encrypted before it is written to disk. Responsibility for
  key management is standardized across all applications and provided as part of the
  infrastructure.
Data in Transit Security:
- Data transmitted over the wire is neither encoded not encrypted.
- Data transmitted over the wire is encrypted for services exposed to the public internet.
  Internal services are not encrypted yet.
- Data transmitted over the wire is encrypted for all services without exception using
  standard methods such as TLS.
- Data transmitted over the wire is encrypted for all services without exception.
  Key and certificate management is a team responsibility.
- Data transmitted over the wire is encrypted for all services without exception.
  Key and certificate management is provided by the infrastructure.
Authentication & Authorization:
- Authentication and authorization between services and applications is not mandatory.
- Both authentication and authorization are a team responsibility. In addition, authentication
  and authorization methods are a team responsibility.
- Both authentication and authorization are a team responsibility. The exact methods
  for authentication and authorization are standardized, but individual teams are
  responsible for implementation.
- Both authentication and authorization is a centrally provided capability. The exact
  methods for authentication and authorization are standardized, but individual teams
  are responsible for implementation.
- Authentication is a centrally provided, standardized capability. However, authorization
  is an individual service responsibility. The exact methods for authentication and
  authorization are standardized, but individual teams are responsible for implementation.
Secrets Management:
- Secrets are embedded within application deployment artifacts and are managed manually.
- Secrets are externalized from application deployment artifacts, but are managed
  manually.
- Secrets are externalized from application deployment artifacts, with encrypted secrets
  being stored in source control.
- Secrets are externalized from application deployment artifacts, with encrypted secrets
  being stored in a configuration management system.
- Secrets are externalized from application deployment artifacts, with encrypted secrets
  being stored in a secrets management server.
